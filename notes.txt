columns to drop
=====

'Record_ID', 'Physician_ID', 'set', 'First_Name', 'Middle_Name', 'Last_Name', 'Name_Suffix', 'Company_Name',  'Product_Code_1',
       'Product_Code_2', 'Product_Code_3', 'Product_Type_1', 'Product_Type_2',
       'Product_Type_3', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3', 'Province'


'Record_ID', 'Physician_ID', 'Company_ID',
       'Total_Amount_of_Payment_USDollars', 'Date', 'Number_of_Payments',
       'Form_of_Payment_or_Transfer_of_Value',
       'Nature_of_Payment_or_Transfer_of_Value', 'City_of_Travel',
       'State_of_Travel', 'Country_of_Travel', 'Ownership_Indicator',
       'Third_Party_Recipient', 'Charity', 'Third_Party_Covered',
       'Contextual_Information', 'Related_Product_Indicator', 'Product_Code_1',
       'Product_Code_2', 'Product_Code_3', 'Product_Type_1', 'Product_Type_2',
       'Product_Type_3', 'Product_Name_1', 'Product_Name_2', 'Product_Name_3',
       'Product_Category_1', 'Product_Category_2', 'Product_Category_3', 'set',
       'First_Name', 'Middle_Name', 'Last_Name', 'Name_Suffix', 'City',
       'State_Phys', 'Zipcode', 'Country_Phys', 'Province',
       'Primary_Specialty', 'License_State_1', 'License_State_2',
       'License_State_3', 'License_State_4', 'License_State_5', 'Company_Name',
       'State_Comp', 'Country_Comp'

NOTES:
Ownership interest reflected in contextual info

BASIS OF DROPPING RECORDS
--Product code present but no name

DROPS BASED ON CORR
=====
All Product Type columns dropped as high Corr with related product indicator
this correlation isnt as significant when na encoded

THINGS TO TRY
======
-with day
-squeeze and drop na
-other models
-increase data size (used only balanced ~4800 samples)
---derived units to try
---weekday
---total amount
-try boosting
-try clustering
-anindya bhai will explore NNs



17-01-2021

further columns to drop
Product Type all three
Product Code all three

Record drop
====
#iterate through and drop all record with no name but code, since these are all for
#0 Ownership_Indicator, can lead to balanced data


Accuracy Notes
============
without column drop:
Internal Accuracy Score 0.985911818418993
RF accuracy: TRAINING 1.0
RF accuracy: TESTING 0.9887179487179487
physician level: 98.22064056939502


after dropping records
Internal Accuracy Score 0.983946141895391
RF accuracy: TRAINING 1.0
RF accuracy: TESTING 0.9915433403805497
phys level: 98.91107078039929


Internal Accuracy Score 0.9820125130344108
RF accuracy: TRAINING 1.0
RF accuracy: TESTING 0.9958847736625515
phys level: 99.45054945054946

random seeds:
756 2434 1247 1059 1493 0.9950592885375494 N_EST: 79
2427 1958 1810 1754 2236 0.9957264957264957 N_EST: 67

533 2608 1581 2218 641 0.9968814968814969 N_EST: 63
821 2128 1239 2803 1151 0.9957850368809273 N_EST: 123

62 2932 1879 2866 830 0.9967320261437909 N_EST: 165
1928 46 2871 2563 1135 0.9968782518210197 N_EST: 68

473 907 1680 322 318 0.9979529170931423 N_EST: 142

SQUEEZING NOTES
======
USE pd.wide_to_long
afterwards, drop all record where *%*&^.Number is not the same.